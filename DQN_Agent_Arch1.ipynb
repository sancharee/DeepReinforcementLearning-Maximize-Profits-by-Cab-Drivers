{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_Agent_Arch1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TIs2csPEfk2J"},"source":["### Cab-Driver Agent"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBLr1jrgcqg1","executionInfo":{"status":"ok","timestamp":1629375548219,"user_tz":-330,"elapsed":36278,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}},"outputId":"fec18a16-c691-4186-e77c-d0a025e98a87"},"source":["\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lLUcMgEqc3s8","executionInfo":{"status":"ok","timestamp":1629375548220,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["import sys\n","sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/DeepRL/RLProjectCodeStructure')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMF6T2Omfk2M","executionInfo":{"status":"ok","timestamp":1629375552756,"user_tz":-330,"elapsed":4540,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["# Importing libraries\n","import numpy as np\n","import random\n","import math\n","from collections import deque\n","import collections\n","import pickle\n","import time\n","\n","# for building DQN model\n","from tensorflow.keras import layers\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","# for plotting graphs\n","import matplotlib.pyplot as plt\n","\n","# Import the environment\n","from Env import CabDriver"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzsTRtUQi-vY","executionInfo":{"status":"ok","timestamp":1629375552757,"user_tz":-330,"elapsed":37,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}},"outputId":"231143ce-4fb8-429c-b315-91d4ab41da3c"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/DeepRL/RLProjectCodeStructure\")\n","!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["DQN_Agent_Arch1.ipynb  Env.py\t\tmodel_weight.pkl  TM.npy\n","DQN_Agent_Arch2.ipynb  model_weight.h5\t__pycache__\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mnVWl8QWfk2d"},"source":["#### Defining Time Matrix"]},{"cell_type":"code","metadata":{"id":"9qpHVtTofk2g","executionInfo":{"status":"ok","timestamp":1629375553530,"user_tz":-330,"elapsed":777,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["# Loading the time matrix provided\n","Time_matrix = np.load('TM.npy')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-sYnXk9fk2u"},"source":["#### Tracking the state-action pairs for checking convergence\n"]},{"cell_type":"code","metadata":{"id":"i_SmW0jJfk2x","executionInfo":{"status":"ok","timestamp":1629375553531,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["def tracked_states_init():\n","    state_action_pair = [((2, 18, 0), (0, 0)),\n","                         ((4, 21, 5), (1, 3)),\n","                         ((4, 21, 0), (0, 0)),\n","                         ((1, 9, 1), (0, 0)),\n","                         ((4, 0, 2), (3, 2)),\n","                         ((0, 6, 0), (4, 1)),\n","                         ((3, 0, 3), (0, 0)),\n","                         ((1, 10, 0), (3, 1)),\n","                         ((1, 0, 0), (1, 2)),\n","                         ((4, 4, 4), (4, 1))]\n","    for state, action in state_action_pair:\n","        if state not in tracked_states:\n","            tracked_states[state] = dict()\n","        tracked_states[state][action] = list()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjK5e3EDfk29","executionInfo":{"status":"ok","timestamp":1629375553532,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["def update_tracked_states():\n","    for state in tracked_states.keys():\n","        encoded_state = np.array(env.state_encod_arch1(state)).reshape(1,36)\n","        q_predict = agent.model.predict(encoded_state)\n","\n","        for action in tracked_states[state].keys():\n","            for index, a in enumerate(env.action_space):\n","                if action == a:\n","                    action_index = index\n","                    break\n","                    \n","            q_value = q_predict[0][action_index]\n","            tracked_states[state][action].append(q_value)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"zc3G062Bfk3G","executionInfo":{"status":"ok","timestamp":1629375553533,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["#Defining a function to save the Q-dictionary as a pickle file\n","def save_obj(obj, name ):\n","    with open(name + '.pkl', 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIg4ngQefk3N"},"source":["### Agent Class\n","\n","If you are using this framework, you need to fill the following to complete the following code block:\n","1. State and Action Size\n","2. Hyperparameters\n","3. Create a neural-network model in function 'build_model()'\n","4. Define epsilon-greedy strategy in function 'get_action()'\n","5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n","6. Complete the 'train_model()' function with following logic:\n","   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n","      - Initialise your input and output batch for training the model\n","      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n","      - Get Q(s', a) values from the last trained model\n","      - Update the input batch as your encoded state and output batch as your Q-values\n","      - Then fit your DQN model using the updated input and output batch."]},{"cell_type":"code","metadata":{"id":"s-ERSWAvfk3O","executionInfo":{"status":"ok","timestamp":1629375553534,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}}},"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        # Define size of state and action\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        # Write here: Specify you hyper parameters for the DQN\n","        self.discount_factor = 0.95\n","        self.learning_rate = 0.01     \n","        self.epsilon_max = 1\n","        self.epsilon_decay = -0.001\n","        self.epsilon_min = 0.00001\n","        self.epsilon = 1\n","        \n","        self.batch_size = 32        \n","        # create replay memory using deque\n","        self.memory = deque(maxlen=2000)\n","        \n","        # Initialize the value of the states tracked\n","        self.states_tracked = []\n","        \n","        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n","        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n","\n","        # create main model and target model\n","        self.model = self.build_model()\n","\n","        \n","    # approximate Q function using Neural Network\n","    def build_model(self):\n","        model = Sequential()\n","        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n","        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n","        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n","        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n","        model.summary\n","        return model\n","\n","\n","    def get_action(self, state):\n","        \"\"\"Choose a action for a given state based on epsilon greedy strategy\"\"\"\n","        # calculation of decay factor for a given episode is done after each episode\n","        \n","        random_value = np.random.rand()\n","        \n","        possible_requests_index, actions = env.requests(state)\n","        \n","        if random_value > self.epsilon:\n","            # exploitation\n","            state_encod = np.array(env.state_encod_arch1(state)).reshape(1,m+t+d)\n","            predicted_q_values = self.model.predict(state_encod)\n","            filter_q_values = [predicted_q_values[0][i] for i in possible_requests_index]\n","            index_max_q_value = np.argmax(np.array(filter_q_values))\n","            selected_action = actions[index_max_q_value]\n","        else:\n","            # exploration\n","            selected_action = random.choice(actions)\n","        \n","        \n","        for i, action in enumerate(env.action_space):\n","            if selected_action == action:\n","                selected_action_index = i\n","                break\n","        \n","        return selected_action_index, selected_action\n","\n","\n","    def append_sample(self, state, action, reward, next_state, is_terminal):\n","        self.memory.append((state, action, reward, next_state, is_terminal))\n","    \n","    \n","    # pick samples randomly from replay memory (with batch_size) and train the network\n","    def train_model(self):\n","        \n","        if len(self.memory) > self.batch_size:\n","            # Sample batch from the memory\n","            mini_batch = random.sample(self.memory, self.batch_size)\n","            update_output = np.zeros((self.batch_size, self.state_size))\n","            update_input = np.zeros((self.batch_size, self.state_size))\n","            \n","            actions, rewards, is_terminal = [], [], []\n","            \n","            for i in range(self.batch_size):\n","                state, action, reward, next_state, terminal = mini_batch[i]\n","                \n","                state_encod = env.state_encod_arch1(state)\n","                update_input[i] = state_encod\n","                \n","                next_state_encod = env.state_encod_arch1(next_state)\n","                update_output[i] = next_state_encod\n","                \n","                actions.append(action)\n","                rewards.append(reward)\n","                is_terminal.append(terminal)\n","                \n","            # 1. Predict the target from earlier model\n","            target = self.model.predict(update_input)\n","\n","            # 2. Get the target for the Q-network\n","            target_q_value = self.model.predict(update_output)\n","\n","            # 3. Update your 'update_output' and 'update_input' batch\n","            for i in range(self.batch_size):\n","                if is_terminal[i]:\n","                    target[i][actions[i]] = rewards[i]\n","                else:\n","                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_q_value[i])\n","                \n","            # 4. Fit your model and track the loss values\n","            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n","            \n","    def save_tracking_states(self):\n","        # Use the model to predict the q_value of the state we are tacking.\n","        q_value = self.model.predict(self.track_state)\n","        \n","        # Grab the q_value of the action index that we are tracking.\n","        self.states_tracked.append(q_value[0][2])\n","        \n","    def save_test_states(self):\n","        # Use the model to predict the q_value of the state we are tacking.\n","        q_value = self.model.predict(self.track_state)\n","        \n","        # Grab the q_value of the action index that we are tracking.\n","        self.states_test.append(q_value[0][2])\n","\n","#     def save(self, name):\n","#         with open(name, 'wb') as file:  \n","#             pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)\n","\n","\n","    def save(self, name):\n","        self.model.save(name)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2QlwKPffk3U","executionInfo":{"status":"ok","timestamp":1629375559690,"user_tz":-330,"elapsed":6165,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}},"outputId":"71fafca6-4139-4b77-cd47-f84dc3935ea0"},"source":["Episodes = 5000\n","#Episodes = 10\n","env = CabDriver()\n","max_total_time_per_episode = 30 * 24\n","m, t, d = 5, 24, 7\n","state_size = m + t + d\n","\n","action_size = m * (m-1) + 1\n","\n","# Call the DQN agent\n","agent = DQNAgent(action_size=action_size, state_size=state_size)\n","\n","# useful metric for tracking\n","rewards_per_episode = []\n","no_of_rides_per_episode = []\n","total_time_per_episode = []\n","epsilon = []"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4dGNRoafk3Y","executionInfo":{"status":"ok","timestamp":1629375559691,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sancharee Das","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsSfBeXaimenFN3aEmGlyKvCFkojyxOi02490VjQ=s64","userId":"13444037440784623547"}},"outputId":"d6e73cf8-563c-4a87-b570-323d453d55bf"},"source":["tracked_states = dict()\n","tracked_states_init()\n","tracked_states"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 6, 0): {(4, 1): []},\n"," (1, 0, 0): {(1, 2): []},\n"," (1, 9, 1): {(0, 0): []},\n"," (1, 10, 0): {(3, 1): []},\n"," (2, 18, 0): {(0, 0): []},\n"," (3, 0, 3): {(0, 0): []},\n"," (4, 0, 2): {(3, 2): []},\n"," (4, 4, 4): {(4, 1): []},\n"," (4, 21, 0): {(0, 0): []},\n"," (4, 21, 5): {(1, 3): []}}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"lcsjBC_tfk3d"},"source":["### DQN block"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0QxaWawOfk3e","outputId":"dd74f073-a366-4165-b546-35f85aded76d"},"source":["start_time = time.time()\n","\n","for episode in range(Episodes):\n","\n","    # Call the environment\n","    env = CabDriver()\n","    action_space, state_space, state = env.reset()\n","\n","    terminal_state = False\n","    total_time = 0\n","    total_reward = 0\n","    no_of_rides = 0\n","    \n","    while terminal_state == False:\n","        \n","        # 1. Pick epsilon-greedy action from possible actions for the current state\n","        action_index, action = agent.get_action(state)\n","        \n","        # 2. Evaluate your reward and next state\n","        next_state, reward, duration = env.next_state_func(state, action, Time_matrix)\n","        total_time += duration\n","        total_reward += reward\n","        \n","        if total_time >= max_total_time_per_episode:\n","            terminal_state = True\n","        \n","        # 3. Append the experience to the memory\n","        # print(\"Episode:{}, State:{}, Action:{}, Reward:{}, NextState:{}, Time:{}\".format(episode, state, action, total_reward, next_state, total_time))\n","        agent.append_sample(state, action_index, reward, next_state, terminal_state)\n","        \n","        # 4. Train the model by calling function agent.train_model\n","        agent.train_model()\n","        \n","        state = next_state\n","        no_of_rides += 1\n","        \n","    # 5. Keep a track of rewards, Q-values, loss\n","    rewards_per_episode.append(total_reward)\n","    no_of_rides_per_episode.append(no_of_rides)\n","    total_time_per_episode.append(total_time)\n","    \n","    # decay in epsilon after each episode\n","    agent.epsilon = (agent.epsilon_max - agent.epsilon_min) * np.exp(agent.epsilon_decay*episode)\n","    epsilon.append(agent.epsilon)\n","    \n","    update_tracked_states()\n","    \n","    if episode % 500 == 0:\n","        print(\"Episode:{} Reward:{} Rides:{} Duration of Episode:{}\" \\\n","              .format(episode, total_reward, no_of_rides, total_time))\n","        agent.save_tracking_states()\n","    \n","    if episode % 1000 == 0:\n","        print(\"Saving Model {}\".format(episode))\n","        agent.save(name=\"model_weight.h5\")\n","\n","agent.save(name=\"model_weight.pkl\")\n","elapsed_time = time.time() - start_time\n","print(\"Elapsed time :\", elapsed_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Episode:0 Reward:-120 Rides:140 Duration of Episode:726\n","Saving Model 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wOwuDNdQfk3j"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vBX77YSQfk3o"},"source":["### Tracking Convergence"]},{"cell_type":"code","metadata":{"id":"UJx7D8Xffk3p"},"source":["for state in tracked_states.keys():\n","    for action in tracked_states[state].keys():\n","        q_values_track_count = len(list(filter(lambda x: x!= 0, tracked_states[state][action])))\n","        print('state:{} action:{} count:{}'.format(state, action, q_values_track_count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVjXP0Nlfk3t"},"source":["tracked_states_plotting = [((4, 21, 5), (1, 3)),\n","                         ((4, 0, 2), (3, 2)),\n","                         ((0, 6, 0), (4, 1)),\n","                         ((1, 10, 0), (3, 1))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWAbC1qdfk3y"},"source":["def convergence_graph_plot(state, action):\n","\n","    tracked_q_values = tracked_states[state][action]\n","    number_of_tracked_episodes = len(tracked_q_values)\n","    plt.plot(range(0, number_of_tracked_episodes), tracked_q_values)\n","        \n","    plt.ylabel(\"Q_value\")\n","    \n","    plt.title(\"State : {} - Action : {}\".format(state, action))\n","    plt.legend([\"Q-value\"], loc=\"lower right\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTmPAgjXfk33"},"source":["def log_convergence_graph_plot(state, action):\n","\n","    tracked_q_values = tracked_states[state][action]\n","    number_of_tracked_episodes = len(tracked_q_values)\n","    plt.semilogy(range(0, number_of_tracked_episodes), tracked_q_values)\n","        \n","    plt.ylabel(\"Q_value\")\n","    \n","    plt.title(\"State : {} - Action : {}\".format(state, action))\n","    plt.legend([\"Q-value\"], loc=\"lower right\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kP4ePvRffk37"},"source":["plt.figure(0, figsize=(20, 15))\n","\n","fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n","fig.tight_layout()\n","\n","subplots = [i for i in range(221, 225)]\n","\n","i = 0\n","\n","for state, action in tracked_states_plotting:\n","    if i < 4:\n","        plt.subplot(subplots[i])\n","        convergence_graph_plot(state, action)\n","        i +=1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mUGaHQcfk4A"},"source":["plt.figure(0, figsize=(20, 15))\n","\n","fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n","fig.tight_layout()\n","\n","subplots = [i for i in range(221, 225)]\n","\n","i = 0\n","\n","for state, action in tracked_states_plotting:\n","    if i < 4:\n","        plt.subplot(subplots[i])\n","        log_convergence_graph_plot(state, action)\n","        i +=1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9S1XoQBzfk4E"},"source":["avg_monthly_rewards = [\n","    np.mean(rewards_per_episode[0:x + 99])\n","    for x in range(0, 15000, 100)\n","]\n","\n","plt.figure(figsize=(20, 10))\n","\n","plt.plot(range(0, len(avg_monthly_rewards)), avg_monthly_rewards)\n","\n","plt.xlabel(\"Number of hunderd(s) episodes (x 100)\")\n","plt.ylabel(\"Average Reward after episodes\")\n","plt.title(\"Reward Graph\")\n","plt.xticks(range(0, 150, 5))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yw9e5763ffzq"},"source":["check epsilon decay over episodes"]},{"cell_type":"code","metadata":{"id":"YNOi6XKVfieX"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","epsilon = []\n","max_epsilon = 1.0\n","min_epsilon = 0.00001\n","\n","episodes = np.arange(0, 5000)\n","\n","for i in episodes:\n","    epsilon.append(min_epsilon +\n","                   (max_epsilon - min_epsilon) * np.exp(-0.0009 * i))\n","\n","plt.plot(episodes, epsilon)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-6QUBjKfg1K"},"source":[""],"execution_count":null,"outputs":[]}]}